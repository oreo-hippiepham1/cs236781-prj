{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is the theoretical part of the final project. It includes theoretical questions from various topics covered in the course.\n",
    "* There are 7 questions among which you need to choose 6, according to the following key:\n",
    "    + Question 1 is **mandatory**.\n",
    "    + Choose **one question** from questions 2-3.\n",
    "    + Question 4 is **mandatory**.\n",
    "    + Questions 5-6 are **mandatory**.\n",
    "    + Question 7 is **mandatory**.\n",
    "* Question 1 is worth 15 points, whereas the other questions worth 7 points.\n",
    "* All in all, the maximal grade for this parts is 15+7*5=50 points.\n",
    "* **You should answer the questions on your own. We will check for plagiarism.**\n",
    "* If you need to add external images (such as graphs) to this notebook, please put them inside the 'imgs' folder. DO NOT put a reference to an external link.\n",
    "* Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: General understanding of the course material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Relate the number of parameters in a neural network to the over-fitting phenomenon (*).\n",
    "    Relate this to the design of convolutional neural networks, and explain why CNNs are a plausible choice for an hypothesis class for visual classification tasks.\n",
    "\n",
    "    (*) In the context of classical under-fitting/over-fitting in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs are a plausible choice for an hypothesis class for visual classification tasks and avoid over-fitting because:\n",
    "\n",
    "- CNNs use shared weights across convolutional layers to recognize fundamental characteristics (edges, corners, and textures) independently of where they are in the picture. This technique is known as \"hared parameter learning.\" In comparison to fully connected layers, this drastically reduces the number of parameters, which makes the network less prone to overfitting, especially in situations when there is little training data.\n",
    "- CNNs are constructed with a number of layers that extract more complicated features. Higher layers combine these elements from lower layers to depict more complex patterns. Lower layers collect fundamental features. This hierarchical feature extraction assists the network in concentrating on pertinent data while ignoring minute differences that can result in overfitting.\n",
    "- CNNs frequently use pooling layers to reduce the spatial dimensions of feature maps by downsampling them. The network can more easily generalize to changes in object location, size, and orientation thanks to this procedure, which creates a type of spatial invariance.\n",
    "- As a result of weight sharing and pooling, CNNs are made to be translation-invariant. This characteristic is advantageous for visual tasks in which the identification of an item is unaffected by its location within a picture.\n",
    "\n",
    "This constraint on the search space of the network prevents over-fitting as the number of parameters increases..ntity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider the linear classifier model with hand-crafted features: \n",
    "    $$f_{w,b}(x) = w^T \\psi(x) + b$$\n",
    "    where $x \\in \\mathbb{R}^2$, $\\psi$ is a non-learnable feature extractor and assume that the classification is done by $sign(f_{w,b}(x))$. Let $\\psi$ be the following feature extractor $\\psi(x)=x^TQx$ where $Q \\in \\mathbb{R}^{2 \\times 2}$ is a non-learnable positive definite matrix. Describe a distribution of the data which the model is able to approximate, but the simple linear model fails to approximate (hint: first, try to describe the decision boundary of the above classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distribution of the data which the model is able to approximate, but the simple linear model fails to approximate is Annular Data Distribution\n",
    "\n",
    "Imagine a dataset with points grouped in circular rings of varied widths and following an annular distribution in a 2D space. Without feature extraction, a straightforward linear classifier will fail to accurately divide the classes using a single straight line. However, by applying feature extraction with the matrix, we can transform the data so that a linear classifier in the transformed space can roughly approach a circular decision boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Assume that we would like to train a Neural Network for classifying images into $C$ classes. Assume that the architecture can be stored in the memory as a computational graph with $N$ nodes where the output is the logits (namely, before applying softmax) for the current batch ($f_w: B \\times Ch \\times H \\times W \\rightarrow B \\times C$). Assume that the computational graph operates on *tensor* values.\n",
    "    * Implement the CE loss assuming that the labels $y$ are hard labels given in a LongTensor (as usual). **Use Torch's log_softmax and index_select functions** and implement with less as possible operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import log_softmax\n",
    "from torch import index_select\n",
    "# Input:  model, x, y. \n",
    "# Output: the loss on the current batch.\n",
    "logits = model(x)\n",
    "...\n",
    "loss = -log_softmax(logits, dim=1).index_select(-1,y).diag().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the model's function as a black box, draw the computational graph (treating both log_softmax and index_select as an atomic operations). How many nodes are there in the computational graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x -> model -> log_softmax -> index_select -> sum -> loss\n",
    "\n",
    "There are 4 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, instead of using hard labels, assume that the labels are representing some probability distribution over the $C$ classes. How would the gradient computation be affected? analyze the growth in the computational graph, memory and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Computation:**\n",
    "\n",
    "When the labels are representing some probability distribution over the $C$ classes (probability distributions), both the predicted distribution and the target distribution are used in the gradient computation. The difference between these distributions is taken into account by the gradient update. Therefore, the gradient updates depend on how well the projected probabilities match the target distribution, rather than only on whether or not the prediction is accurate.\n",
    "\n",
    "**Computational Complexity and Memory:**\n",
    "\n",
    "This can lead to increased in computation and memory usage because:\n",
    "\n",
    "1. **Divergence Computation:** As opposed to the conventional cross-entropy loss with hard labels, calculating divergences (such as the KL divergence) between projected probability and target probabilities requires extra calculations. We would have to multiply each log_softmax by its corresponding target distribution, which would require extra processing.\n",
    "\n",
    "2. **Memory Usage:** When using labels with probability distributions, it is necessary to store probability distributions rather than just single numbers for each sample. Higher memory utilization might come from this, especially for bigger datasets.\n",
    "\n",
    "3. **Additional Gradients:** Additional gradients must be calculated during backpropagation for both predicted probability (p) and target probabilities (q). Additional memory and processing power are needed for this.\n",
    "\n",
    "**Growth in Computational Graph:**\n",
    "\n",
    "Divergence calculations, which cover terms pertaining to both the anticipated and target distributions, add more terms to the computational graph, making it more complex. When there are more classes, the graph's complexity increase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply the same analysis in the case that we would like to double the batch size. How should we change the learning rate of the optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Computation:**\n",
    "The gradients calculated in each iteration will be based on a bigger subset of the dataset when the batch size is doubled. As a result, gradient estimations may be more precise and convergence may be more stable. \n",
    "\n",
    "**Memory Usage:**\n",
    "For both forward and backward passes, doubling the batch size increases the amount of memory needed to store the intermediate activations and gradients. This is because more data must be processed on each cycle, which calls for more memory.\n",
    "\n",
    "**Growth in Computational Graph:**\n",
    "No growth in computational graph and complexity.\n",
    "\n",
    "**Learning Rate Adjustment:**\n",
    "The learning rate should normally be adjusted to maintain a constant rate of parameter updates and convergence when the batch size is doubled. To get the same impact on the model's weights, the learning rate should be scaled proportionally to the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Optimization & Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: resolving gradient conflicts in multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that you want to train a model to perform two tasks: task 1 and task 2. \n",
    "For each such task $i$ you have an already implemented function *loss\\_i = forward_and_compute_loss_i(model,inputs)* such that given the model and the inputs it computes the loss w.r.t task $i$ (assume that the computational graph is properly constructed). We would like to train our model using SGD to succeed in both tasks as follows: in each training iteration (batch) -\n",
    "* Let $g_i$ be the gradient w.r.t the $i$-th task.\n",
    "* If $g_1 \\cdot g_2 < 0$:\n",
    "    + Pick a task $i$ at random.\n",
    "    + Apply GD w.r.t only that task.\n",
    "* Otherwise:\n",
    "    + Apply GD w.r.t both tasks (namely $\\mathcal{L}_1 + \\mathcal{L}_2$).\n",
    "\n",
    "Note that in the above formulation the gradient is a thought of as a concatination of all the gradient w.r.t all the models parameters, and $g_1 \\cdot g_2$ stands for a dot product.\n",
    "\n",
    "What parts should be modified to implement the above? Is it the optimizer, the training loop or both? Implement the above algorithm in a code cell/s below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # We only need to modify the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['inputs']\n",
    "        \n",
    "        # Compute gradients for both tasks\n",
    "        optimizer.zero_grad()\n",
    "        loss_1 = forward_and_compute_loss_1(model, inputs)\n",
    "        loss_2 = forward_and_compute_loss_2(model, inputs)\n",
    "        loss_total = loss_1 + loss_2\n",
    "        # Compute dot product of gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss_1.backward()\n",
    "        grad_1 = torch.cat([p.grad.view(-1) for p in model.parameters()])\n",
    "        optimizer.zero_grad()\n",
    "        loss_2.backward()\n",
    "        grad_2 = torch.cat([p.grad.view(-1) for p in model.parameters()])\n",
    "        dot_product = torch.dot(grad_1, grad_2)\n",
    "        if dot_product < 0:\n",
    "            # Pick a task at random\n",
    "            if torch.rand(1) < 0.5:\n",
    "                optimizer.zero_grad()\n",
    "                loss_1.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                # Apply GD w.r.t only the other task\n",
    "                optimizer.zero_grad()\n",
    "                loss_2.backward()\n",
    "                optimizer.step()\n",
    "        else:\n",
    "            # Apply GD w.r.t both tasks\n",
    "            optimizer.zero_grad()\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: manual automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following two-input two-output function:\n",
    "$$ f(x,y) = (x^2\\sin(xy+\\frac{\\pi}{2}), x^2\\ln(1+xy)) $$\n",
    "* Draw a computational graph for the above function. Assume that the unary atomic units are squaring, taking square root, $\\exp,\\ln$, basic trigonometric functions and the binary atomic units are addition and multiplication. You would have to use constant nodes.\n",
    "* Calculate manually the forward pass.\n",
    "* Calculate manually the derivative of all outputs w.r.t all inputs using a forward mode AD.\n",
    "* Calculate manually the derivative of all outputs w.r.t all inputs using a backward mode AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: RNNs vs Transformers in the real life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each one of the following scenarios decide whether to use RNN based model or a transformer based model. Justify your choice.\n",
    "1. You are running a start-up in the area of automatic summarization of academic papers. The inference of the model is done on the server side, and it is very important for it to be fast.\n",
    "2. You need to design a mobile application that gathers small amount of data from few apps in every second and then uses a NN to possibly generate an alert given the information in the current second and the information from the past minute.\n",
    "3. You have a prediction task over fixed length sequences on which you know the following properties:\n",
    "    + In each sequence there are only few tokens that the model should attend to.\n",
    "    + Most of the information needed for generating a reliable prediction is located at the beginning of the sequence.\n",
    "    + There is no restriction on the computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario 1: Automatic Summarization of Academic Papers\n",
    "\n",
    "A transformer-based approach might be better appropriate for this situation.\n",
    "Transformers can efficiently extract contextual information from various document sections, which is crucial for producing reliable and cogent summaries.\n",
    "\n",
    "Scenario 2: Mobile Application for Real-time Data and Alerts\n",
    "\n",
    "RNN-based model, like as LSTM or GRU, might be more suited in this situation. RNNs can capture temporal relationships in the data and intuitively handle sequential data.\n",
    "RNNs can effectively analyze this temporal component since the program collects data every second and leverages data from the previous minute.\n",
    "Compared to transformers, RNNs have fewer parameters, which is beneficial for real-time processing on a mobile device with constrained resources.\n",
    "\n",
    "Scenario 3: Prediction with Fixed Length Sequences\n",
    "\n",
    "An RNN-based model appears to be appropriate for this circumstance based on the information supplied.\n",
    "The small number of important tokens in each sequence emphasizes the need for an attention mechanism, which RNNs may offer.\n",
    "The sequence's beginning contains the majority of the data required for precise predictions, which is consistent with RNNs' capacity to understand the sequence's beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Generative modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: VAEs and GANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest a method for combining VAEs and GANs. Focus on the different components of the model and how to train them jointly (the objectives). Which drawbacks of these models the combined model may overcome? which not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A method for combining VAEs and GANs is the AAEs:\n",
    "\n",
    "1. **Architecture:**\n",
    "   - AAEs combine the adversarial training of GANs with the encoder and decoder structure of VAEs.\n",
    "   - Similar to a VAE, the encoder maps input data to a latent space.\n",
    "   - Similar to a VAE, the generator (decoder) creates data samples from latent codes.\n",
    "   - To discriminate between latent codes taken from the data distribution and those from the encoder, a discriminator, as used in GANs, is introduced instead of using the KL divergence.\n",
    "\n",
    "2. **Prior Matching:**\n",
    "    - AAEs use the discriminator to match the approximate posterior to the prior of the genuine data distribution rather than forcing the approximate posterior (encoder output) to match a unit Gaussian. This enables the AAE to learn a continuous and more precise representation of latent space.\n",
    "\n",
    "3. **Adversarial Training:**\n",
    "   - The discriminator is trained to distinguish between samples from the aggregated approximate posterior (VAE encoder) and samples from the true data latent code distribution.\n",
    "\n",
    "Drawbacks Addressed by AAEs:\n",
    "\n",
    "1. **Latent Space Quality:**\n",
    "    - AAEs improve the continuous and smooth nature of the learnt latent space to address the issue of blurry or low-quality samples produced by VAEs. AAEs make sure that the latent space better captures the data manifold by comparing the aggregated posterior to the prior of the real data distribution.\n",
    "\n",
    "2. **Data Distribution Capture:**\n",
    "    - By training the model to match the previous distribution of the true data distribution, AAEs overcome the shortcoming of VAEs in capturing complicated data distributions. This allows the AAE to more correctly identify the modes and fluctuations in the data distribution, which improves sample quality.\n",
    "\n",
    "Drawbacks AAEs Might Not Fully Overcome:\n",
    "\n",
    "1. **Inference Speed:**\n",
    "    -  AAEs and VAEs both use encoder-decoder structures, which may not be able to meet the theoretical speed requirement indicated in some cases.\n",
    "    - In situations demanding quick inference, transformers—known for their parallelization abilities—might nevertheless perform better than both VAEs and AAEs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that $q(x_{t-1}|x_t,x_0)$ is tractable and is given by $\\mathcal{N}(x_{t-1};\\tilde{\\mu}(x_t,x_0),\\tilde{\\beta_t}I)$ where the terms for $\\tilde{\\mu}(x_t,x_0)$ and $\\tilde{\\beta_t}$ are given in the last tutorial. Do so by explicitly computing the PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a forward noise process $q$ that, given a data distribution $x_0 sim q(x_0)$, generates latents $x_1$ through $x_T$ by adding Gaussian noise at time $t$ with variance $beta_t \\in (0, 1)$ as follows:\n",
    "$$\n",
    "q(x_1, ..., x_T | x_0) = \\prod_{t=1}^{T} q(x_t | x_{t-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "q(x_t | x_{t-1}) = \\mathcal{N} \\left( x_t; (1 - \\beta_t) x_{t-1}, \\beta_t I \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The forward process allow us to compute $q(x_t|x_0)$ directly:\n",
    "$$ q(x_t|x_0) = \\mathcal{N}(x_t;\\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha_t})\\mathbf{I})$$\n",
    "* where $\\alpha_t := 1- \\beta_t$ and $\\bar{\\alpha_t} = \\prod_{t=1}^T \\alpha_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bayes' theorem, we compute the posterior $q(x_t-1|x_t, x_0)$:\n",
    "   \n",
    "   $$p(x_{t-1}|x_t, x_0) = \\frac{p(x_t, x_{t-1} | x_0)}{p(x_t | x_0)}$$\n",
    "\n",
    "   $$p(x_{t-1}|x_t, x_0) = \\frac{q(x_t | x_{t-1}, x_0) \\cdot p(x_{t-1} | x_0)}{p(x_t | x_0)}$$\n",
    "Using markov assumption $ q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) $ we get:\n",
    "   $$p(x_{t-1}|x_t, x_0) = \\frac{q(x_t | x_{t-1}) \\cdot p(x_{t-1} | x_0)}{p(x_t | x_0)}$$\n",
    "   $$q(x_{t-1}|x_t, x_0) = \\frac{\\mathcal{N}(x_t; \\tilde{\\mu}_t(x_{t-1}, x_0), \\beta_t I) \\cdot \\mathcal{N}(x_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} x_0, (1 - \\bar{\\alpha}_{t-1}) I)}{\\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)}$$\n",
    "   \n",
    "The resulting posterior distribution is also a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we get\n",
    "\n",
    "$$ q(x_{t-1}|x_t,x_0) = \\mathcal{N}(x_{t-1}; \\tilde{\\mu}(x_t,x_0),\\tilde{\\beta_t}\\mathbf{I})$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$ \\tilde{\\mu}(x_t,x_0) := \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1-\\bar{\\alpha}_t}x_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t$$\n",
    "\n",
    "and:\n",
    "$$\\tilde{\\beta}_t := \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\beta_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Batch Normalization and Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both BatchNorm and Dropout analyze the following:\n",
    "1. How to use them during the training phase (both in forward pass and backward pass)?\n",
    "2. How differently they behave in the inference phase? How to distinguish these operation modes in code?\n",
    "3. Assume you would like to perform multi-GPU training (*) to train your model. What should be done in order for BatchNorm and dropout to work properly? assume that each process holds its own copy of the model and that the processes can share information with each other.\n",
    "\n",
    "(*): In a multi-GPU training each GPU is associated with its own process that holds an independent copy of the model. In each training iteration a (large) batch is split among these processes (GPUs) which compute the gradients of the loss w.r.t the relevant split of the data. Afterwards, the gradients from each process are then shared and averaged so that the GD would take into account the correct gradient and to assure synchornization of the model copies. Note that the proccesses are blocked between training iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To use them during the training phase:\n",
    "\n",
    "    **BatchNorm**\n",
    "    - Forward Pass: Activations within each mini-batch are normalized by BatchNorm during training, which centers them around zero and scales them using the learnt parameters (mean and variance).\n",
    "    - Backward Pass: During backpropagation, gradients propagate via normalized activations and update the mean, variance, and model weights.\n",
    "  \n",
    "   **Dropout**\n",
    "    - Forward Pass: Dropout randomly deactivates neurons with a predetermined probability during training to minimize co-dependencies and avoid overfitting.\n",
    "    - Backward Pass: During backpropagation, gradients only pass via neurons that are activated.\n",
    "  \n",
    "2.   Different behavior in the inference phase:\n",
    "    - Dropout is disabled and all neurons are engaged during inference. To take training-time dropout into account, outputs are scaled by dropout probability. while Batch Norm is still active during inference.\n",
    "    - To distinguish these operation modes use model.eval() for inference and model.train() for training.\n",
    "4. For proper BatchNorm performance in multi-GPU configurations we need to independently calculate statistics on each GPU, sync statistics between GPUs and perform BatchNorm across all GPU. To guarantee consistency in the results for Drop out, ensure consistent dropout probabilities across all GPUs.rs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
